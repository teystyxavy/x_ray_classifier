{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0ecc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 20 17:21:35 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.80                 Driver Version: 576.80         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   59C    P8              5W /   10W |     323MiB /   6141MiB |     85%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           19080    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8e0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "import tkinter as tk\n",
    "import time\n",
    "import math\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b873bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "1\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#check whether cuda is enabled\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# set device to cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eeb6063",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['left_AP', 'right_AP', 'left_outlet', 'right_outlet']\n",
    "# define segmentation method\n",
    "def crop_segments(results, image):\n",
    "    '''\n",
    "    crop out the segment of the image\n",
    "\n",
    "    Args:\n",
    "        results: the results of the segmentation\n",
    "        image: the image\n",
    "    \n",
    "    return:\n",
    "        cropped_segments: list containing set of cropped segment information\n",
    "    '''\n",
    "    cropped_segments = []\n",
    "    for result in results:\n",
    "        if result.masks is not None:\n",
    "            masks = result.masks.data.cpu().numpy()\n",
    "            \n",
    "            for i, mask in enumerate(masks):\n",
    "                 # resize mask to match image dimensions\n",
    "                if mask.shape != image.shape[:2]:\n",
    "                    mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
    "\n",
    "                # find bounding box of the mask\n",
    "                coords = np.column_stack(np.where(mask > 0.5))\n",
    "                if len(coords) > 0:\n",
    "                    y1, x1 = coords.min(axis=0)\n",
    "                    y2, x2 = coords.max(axis=0)\n",
    "\n",
    "                    # crop the region\n",
    "                    cropped_image = image[y1:y2+1, x1:x2+1]\n",
    "                    cropped_mask = mask[y1:y2+1, x1:x2+1]\n",
    "\n",
    "                    # Create image w black background\n",
    "                    masked_image = cropped_image.copy()\n",
    "                    # Set background pixels (where mask is False) to black\n",
    "                    masked_image[cropped_mask <= 0.5] = [0, 0, 0]\n",
    "\n",
    "                    cropped_segments.append({\n",
    "                        'image': masked_image,\n",
    "                        'original': cropped_image,\n",
    "                        'mask': cropped_mask,\n",
    "                        'index': i,\n",
    "                        'bbox': (x1, y1, x2, y2)\n",
    "                    })\n",
    "    return cropped_segments\n",
    "\n",
    "# save image method\n",
    "\n",
    "def save_cropped_segments(segments, image_name, output_dir):\n",
    "    '''save cropped segments to output_dir'''\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for segment in segments:\n",
    "        filename = image_name.split('.')[0] + '_mask' + '.png'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Save image with black background\n",
    "        cv2.imwrite(filepath, segment['image'])\n",
    "        print(f'Saved: {filepath}')\n",
    "\n",
    "def resize_with_aspect_ratio(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    '''resize image with aspect ratio'''    \n",
    "    # get the current width and height\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    \n",
    "    # calculate the scaling factor\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    if width is None:   \n",
    "        scale = height / h\n",
    "        dim = (int(w * scale), height)\n",
    "    else:\n",
    "        scale = width / w\n",
    "        dim = (width, int(h * scale))\n",
    "\n",
    "    return cv2.resize(image, dim, interpolation=inter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18132a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine test frames to form a new video\n",
    "full_img_paths = []\n",
    "os.makedirs('test_videos', exist_ok=True)\n",
    "\n",
    "for class_name in classes:\n",
    "    image_folder = os.path.join('test/test', class_name)\n",
    "    curr_images = [img for img in os.listdir(image_folder) if img.endswith('.jpg')]\n",
    "    full_img_paths.extend([os.path.join(image_folder, img) for img in curr_images])\n",
    "\n",
    "image_folder = os.path.join('test/test', classes[0])\n",
    "frame = cv2.imread(full_img_paths[0])\n",
    "height, width, layers = frame.shape\n",
    "video_writer = cv2.VideoWriter('test_videos/test_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 10, (width, height))\n",
    "\n",
    "for img_path in full_img_paths:\n",
    "    frame = cv2.imread(img_path)\n",
    "    if frame is not None:\n",
    "        # Ensure frame has correct dimensions\n",
    "        if frame.shape[:2] == (height, width):\n",
    "            video_writer.write(frame)\n",
    "        else:\n",
    "            # Resize if dimensions don't match\n",
    "            frame_resized = cv2.resize(frame, (width, height))\n",
    "            video_writer.write(frame_resized)\n",
    "    else:\n",
    "        print(f\"Warning: Could not read image {img_path}\")\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Video created with {len(full_img_paths)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8a856",
   "metadata": {},
   "source": [
    "### Realtime Segmentation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae89b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 5 is model path to segmentation model\n",
    "def real_time_segment(input_video, output_video, model_path=None, display_frame=False, conf_threshold=0.5):\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "\n",
    "    assert cap.isOpened(), 'Error reading video file'\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_video), exist_ok=True)\n",
    "\n",
    "    # Video Writer\n",
    "    w, h, fps = (int(cap.get(x))for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "    video_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "\n",
    "\n",
    "    # Initialze instance segmentation object\n",
    "    show = False\n",
    "    if display_frame:\n",
    "        show = True\n",
    "\n",
    "    isegment = solutions.InstanceSegmentation(\n",
    "        show=show, # display output\n",
    "        model=model_path, # path to model weights\n",
    "        device=[0],\n",
    "        conf=conf_threshold,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # process video\n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, im0 = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print('Video frame is empty or video processing successfully completed')\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # process frame\n",
    "            processed_frame = isegment(im0)\n",
    "\n",
    "            # write processed frame to output video\n",
    "            if processed_frame is not None:\n",
    "                video_writer.write(processed_frame.plot_im)\n",
    "                processed_count += 1\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        cap.release()\n",
    "        video_writer.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f'Input frames: {frame_count}')\n",
    "        print(f'Processed frames: {processed_count}')\n",
    "        print(f'Output saved to: {output_video}')\n",
    "        print(f'Inference speed: {((end_time - start_time) * 10**3) / processed_count} ms per frame')\n",
    "        print('Video processing completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2073109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics Solutions:  {'source': None, 'model': 'runs/segment/train5/weights/best.pt', 'classes': None, 'show_conf': True, 'show_labels': True, 'region': None, 'colormap': 21, 'show_in': True, 'show_out': True, 'up_angle': 145.0, 'down_angle': 90, 'kpts': [6, 8, 10], 'analytics_type': 'line', 'figsize': (12.8, 7.2), 'blur_ratio': 0.5, 'vision_point': (20, 20), 'crop_dir': 'cropped-detections', 'json_file': None, 'line_width': 2, 'records': 5, 'fps': 30.0, 'max_hist': 5, 'meter_per_pixel': 0.05, 'max_speed': 120, 'show': False, 'iou': 0.7, 'conf': 0.5, 'device': [0], 'max_det': 300, 'half': False, 'tracker': 'botsort.yaml', 'verbose': False, 'data': 'images'}\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING No masks detected! Ensure you're using a supported Ultralytics segmentation model.\n",
      "WARNING no tracks found!\n",
      "WARNING no tracks found!\n",
      "WARNING no tracks found!\n",
      "Video frame is empty or video processing successfully completed\n",
      "Input frames: 521\n",
      "Processed frames: 521\n",
      "Output saved to: output_videos/test_video2_seg.mp4\n",
      "Inference speed: 108.27372124465093 ms per frame\n",
      "Video processing completed\n"
     ]
    }
   ],
   "source": [
    "real_time_segment(input_video='input_videos/test_video2.mp4', output_video='output_videos/test_video2_seg.mp4', model_path='runs/segment/train5/weights/best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c21f68",
   "metadata": {},
   "source": [
    "### Real Time Detection ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e5ac04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_detect(det_model_path, input_video, output_video, output_fps=None, display_frame=False, conf_threshold=0.25):\n",
    "    det_model = YOLO(det_model_path)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_video), exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    assert cap.isOpened(), 'Error reading video file'\n",
    "\n",
    "    # get video properties\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if output_fps is None:\n",
    "        output_fps = original_fps\n",
    "    \n",
    "\n",
    "    print(f\"Input: {width} x {height} at {original_fps} fps, {total_frames} frames\")\n",
    "    print(f\"Output: {width} x {height} at {output_fps} fps\")\n",
    "\n",
    "    # intialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video, fourcc, output_fps, (width, height))\n",
    "\n",
    "    if not video_writer.isOpened():\n",
    "        print('Error opening video writer')\n",
    "        cap.release()\n",
    "        return\n",
    "    \n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # process video\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print('Video frame is empty or video processing successfully completed')\n",
    "                break\n",
    "                \n",
    "            frame_count += 1\n",
    "\n",
    "            # resize frame if needed\n",
    "            if frame.shape[0] != height or frame.shape[1] != width:\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "\n",
    "            # process frame\n",
    "            results = det_model.predict(frame, verbose=False, conf=conf_threshold) \n",
    "\n",
    "            # draw results on frame\n",
    "            annotated_frame = results[0].plot()\n",
    "\n",
    "            # ensure annotated frame is the same size as the original frame\n",
    "            if annotated_frame.shape[0] != height or annotated_frame.shape[1] != width:\n",
    "                annotated_frame = cv2.resize(annotated_frame, (width, height))\n",
    "\n",
    "            # write frame to output video\n",
    "            video_writer.write(annotated_frame)\n",
    "            processed_count += 1\n",
    "            \n",
    "            # print progress\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {processed_count} frames / {frame_count} total\")\n",
    "            \n",
    "            # Display the frame\n",
    "            if display_frame:\n",
    "                cv2.imshow('YOLO11 Real-time', annotated_frame)\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "                if key == ord('q'):\n",
    "                    print('Processing stopped by user')\n",
    "                    break\n",
    "                elif key == ord('p'):\n",
    "                    print('Processing paused, press any key to resume')\n",
    "                    cv2.waitKey(0)\n",
    "                    print('Processing resumed')\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_count}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        cap.release()\n",
    "        video_writer.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print('Video processing completed')\n",
    "        print(f'Input frames: {frame_count}')\n",
    "        print(f'Processed frames: {processed_count}')\n",
    "        print(f'Output saved to: {output_video}')\n",
    "        print('Inference speed: ', ((end_time - start_time)* 10**3) / frame_count, 'ms/frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "948d73d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1080 x 1920 at 30.21165555233401 fps, 521 frames\n",
      "Output: 1080 x 1920 at 30.21165555233401 fps\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Processed 360 frames / 360 total\n",
      "Processed 390 frames / 390 total\n",
      "Processed 420 frames / 420 total\n",
      "Processed 450 frames / 450 total\n",
      "Processed 480 frames / 480 total\n",
      "Processed 510 frames / 510 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 521\n",
      "Processed frames: 521\n",
      "Output saved to: output_videos/test_video2_det.mp4\n",
      "Inference speed:  38.420639660445374 ms/frame\n"
     ]
    }
   ],
   "source": [
    "real_time_detect(det_model_path='runs/detect/train2/weights/best.pt', input_video='input_videos/test_video2.mp4', output_video='output_videos/test_video2_det.mp4', conf_threshold=0.732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "323d68c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1080 x 1920 at 30.011688030155746 fps, 1035 frames\n",
      "Output: 1080 x 1920 at 30.011688030155746 fps\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Processed 360 frames / 360 total\n",
      "Processed 390 frames / 390 total\n",
      "Processed 420 frames / 420 total\n",
      "Processed 450 frames / 450 total\n",
      "Processed 480 frames / 480 total\n",
      "Processed 510 frames / 510 total\n",
      "Processed 540 frames / 540 total\n",
      "Processed 570 frames / 570 total\n",
      "Processed 600 frames / 600 total\n",
      "Processed 630 frames / 630 total\n",
      "Processed 660 frames / 660 total\n",
      "Processed 690 frames / 690 total\n",
      "Processed 720 frames / 720 total\n",
      "Processed 750 frames / 750 total\n",
      "Processed 780 frames / 780 total\n",
      "Processed 810 frames / 810 total\n",
      "Processed 840 frames / 840 total\n",
      "Processed 870 frames / 870 total\n",
      "Processed 900 frames / 900 total\n",
      "Processed 930 frames / 930 total\n",
      "Processed 960 frames / 960 total\n",
      "Processed 990 frames / 990 total\n",
      "Processed 1020 frames / 1020 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 1035\n",
      "Processed frames: 1035\n",
      "Output saved to: output_videos/test_video3_det.mp4\n",
      "Inference speed:  38.623880303424336 ms/frame\n"
     ]
    }
   ],
   "source": [
    "real_time_detect(det_model_path='runs/detect/train2/weights/best.pt', input_video='input_videos/test_video3.mp4', output_video='output_videos/test_video3_det.mp4', conf_threshold=0.732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74a69324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1080 x 1920 at 30.011688030155746 fps, 1035 frames\n",
      "Output: 1080 x 1920 at 30.011688030155746 fps\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Processed 360 frames / 360 total\n",
      "Processed 390 frames / 390 total\n",
      "Processed 420 frames / 420 total\n",
      "Processed 450 frames / 450 total\n",
      "Processed 480 frames / 480 total\n",
      "Processed 510 frames / 510 total\n",
      "Processed 540 frames / 540 total\n",
      "Processed 570 frames / 570 total\n",
      "Processed 600 frames / 600 total\n",
      "Processed 630 frames / 630 total\n",
      "Processed 660 frames / 660 total\n",
      "Processed 690 frames / 690 total\n",
      "Processed 720 frames / 720 total\n",
      "Processed 750 frames / 750 total\n",
      "Processed 780 frames / 780 total\n",
      "Processed 810 frames / 810 total\n",
      "Processed 840 frames / 840 total\n",
      "Processed 870 frames / 870 total\n",
      "Processed 900 frames / 900 total\n",
      "Processed 930 frames / 930 total\n",
      "Processed 960 frames / 960 total\n",
      "Processed 990 frames / 990 total\n",
      "Processed 1020 frames / 1020 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 1035\n",
      "Processed frames: 1035\n",
      "Output saved to: output_videos/test_video3_det.mp4\n",
      "Inference speed:  39.211460703237044 ms/frame\n"
     ]
    }
   ],
   "source": [
    "real_time_detect(det_model_path='runs/detect/train_light/weights/best.pt', input_video='input_videos/test_video3.mp4', output_video='output_videos/test_video3_det.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a906b62",
   "metadata": {},
   "source": [
    "### Realtime Classification ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop bounding box\n",
    "def crop_bounding_box(results, image):\n",
    "    '''\n",
    "        crops image based on bounding box returned by YOLOv11 model\n",
    "        \n",
    "        Args:\n",
    "            results: YOLOv11 model output\n",
    "            image: image to crop\n",
    "            \n",
    "        Returns:\n",
    "            cropped: cropped image\n",
    "    '''\n",
    "    person_class_id = 0\n",
    "    for result in results:\n",
    "        if result.boxes is not None:\n",
    "            classes = result.boxes.cls.cpu().numpy()\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()\n",
    "\n",
    "            for i, class_id in enumerate(classes):\n",
    "                if int(class_id) == person_class_id:\n",
    "                    x1, y1, x2, y2 = map(int, boxes[i])\n",
    "                    cropped = image[y1:y2, x1:x2]\n",
    "                    return cropped\n",
    "                \n",
    "    print('No person detected in image')\n",
    "    return image\n",
    "\n",
    "# create augmentation pipeline\n",
    "def crop_image(img, crop_percent):\n",
    "    return img[int(crop_percent*img.shape[0]):int((1-crop_percent)*img.shape[0]), int(crop_percent*img.shape[1]):int((1-crop_percent)*img.shape[1])]\n",
    "\n",
    "def draw_line(img):\n",
    "    '''\n",
    "        draws line down middle of image\n",
    "        \n",
    "        Args:\n",
    "            img(cv2 image): image to draw line on\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    '''\n",
    "    # get image dimensions\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # calc middle coordinate\n",
    "    middle_x = w // 2\n",
    "\n",
    "    # draw line down middle\n",
    "    cv2.line(img, (middle_x, 0), (middle_x, h), (0, 255, 0), 2)\n",
    "\n",
    "def real_time_classify(cls_model_path, input_video, output_video, det_model_path=None, output_fps=None, bounding_box_crop=False, \n",
    "                       fixed_crop=False, crop_percent=0.1, line=False, display_frame=False):\n",
    "    '''\n",
    "        Performs real-time classification on video using YOLOv11 model\n",
    "        \n",
    "        Args:\n",
    "            cls_model_path: path to YOLOv11 classification model\n",
    "            det_model_path: path to YOLOv11 detection model\n",
    "            input_video: path to input video\n",
    "            output_video: path to output video\n",
    "            output_fps: output video fps, if None, use original fps\n",
    "            bounding_box_crop: if True, crops bounding box around person, else crops around edges\n",
    "            fixed_crop: if True, crops image to fixed size \n",
    "            crop_percent: percentage of image to crop (default 0.1)\n",
    "            \n",
    "        Returns: \n",
    "            None\n",
    "    '''\n",
    "    # load models\n",
    "    cls_model = YOLO(cls_model_path)\n",
    "\n",
    "    if det_model_path is not None:\n",
    "        det_model = YOLO(det_model_path)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_video), exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    assert cap.isOpened(), 'Error reading video file'\n",
    "\n",
    "    # get video properties\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if output_fps is None:\n",
    "        output_fps = original_fps\n",
    "    \n",
    "\n",
    "    print(f\"Input: {width} x {height} at {original_fps} fps, {total_frames} frames\")\n",
    "    print(f\"Output: {width} x {height} at {output_fps} fps\")\n",
    "\n",
    "    # intialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video, fourcc, output_fps, (width, height))\n",
    "\n",
    "    if not video_writer.isOpened():\n",
    "        print('Error opening video writer')\n",
    "        cap.release()\n",
    "        return\n",
    "    \n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # process video\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print('Video frame is empty or video processing successfully completed')\n",
    "                break\n",
    "                \n",
    "            frame_count += 1\n",
    "\n",
    "            if det_model_path is not None:\n",
    "                # get bounding box results\n",
    "                results = det_model(frame, verbose=False)\n",
    "                frame = crop_bounding_box(results, frame)\n",
    "            elif fixed_crop:\n",
    "                crop_image(frame, crop_percent)\n",
    "\n",
    "            if line:\n",
    "                draw_line(frame)\n",
    "\n",
    "            # resize frame if needed\n",
    "            if frame.shape[0] != height or frame.shape[1] != width:\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "\n",
    "            # process frame\n",
    "            results = cls_model(frame, verbose=False) \n",
    "\n",
    "            # draw results on frame\n",
    "            annotated_frame = results[0].plot()\n",
    "\n",
    "            # ensure annotated frame is the same size as the original frame\n",
    "            if annotated_frame.shape[0] != height or annotated_frame.shape[1] != width:\n",
    "                annotated_frame = cv2.resize(annotated_frame, (width, height))\n",
    "\n",
    "            # write frame to output video\n",
    "            video_writer.write(annotated_frame)\n",
    "            processed_count += 1\n",
    "            \n",
    "            # print progress\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {processed_count} frames / {frame_count} total\")\n",
    "            \n",
    "            if display_frame:\n",
    "                final_frame = resize_with_aspect_ratio(annotated_frame, width=500, height=500)\n",
    "                # Display the frame\n",
    "                cv2.imshow('YOLO11 Real-time', final_frame)\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "                if key == ord('q'):\n",
    "                    print('Processing stopped by user')\n",
    "                    break\n",
    "                elif key == ord('p'):\n",
    "                    print('Processing paused, press any key to resume')\n",
    "                    cv2.waitKey(0)\n",
    "                    print('Processing resumed')\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_count}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        cap.release()\n",
    "        video_writer.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print('Video processing completed')\n",
    "        print(f'Input frames: {frame_count}')\n",
    "        print(f'Processed frames: {processed_count}')\n",
    "        print(f'Output saved to: {output_video}')\n",
    "        print('Inference speed: ', ((end_time - start_time) * 10**3) / frame_count, 'ms/frame')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52fe96c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1080 x 1920 at 30.21165555233401 fps, 521 frames\n",
      "Output: 1080 x 1920 at 30.21165555233401 fps\n",
      "Error processing frame 1: name 'resize_with_aspect_ratio' is not defined\n",
      "Video processing completed\n",
      "Input frames: 1\n",
      "Processed frames: 1\n",
      "Output saved to: output_videos/test_video2_cls_bbox.mp4\n",
      "Inference speed:  672.3356246948242 ms/frame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "real_time_classify(cls_model_path='runs/classify/train96/weights/best.pt', det_model_path='yolo11n.pt', input_video='input_videos/test_video2.mp4', output_video='output_videos/test_video2_cls_bbox.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51fd95bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1080 x 1920 at 30.21165555233401 fps, 521 frames\n",
      "Output: 1080 x 1920 at 30.21165555233401 fps\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Processed 360 frames / 360 total\n",
      "Processed 390 frames / 390 total\n",
      "Processed 420 frames / 420 total\n",
      "Processed 450 frames / 450 total\n",
      "Processed 480 frames / 480 total\n",
      "Processed 510 frames / 510 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 521\n",
      "Processed frames: 521\n",
      "Output saved to: output_videos/test_video2_cls.mp4\n",
      "Inference speed:  56.391574142075314 ms/frame\n"
     ]
    }
   ],
   "source": [
    "real_time_classify(cls_model_path='runs/classify/train80/weights/best.pt', input_video='input_videos/test_video2.mp4', output_video='output_videos/test_video2_cls.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71fca43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1080 x 1920 at 30.011688030155746 fps, 1035 frames\n",
      "Output: 1080 x 1920 at 30.011688030155746 fps\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Processed 360 frames / 360 total\n",
      "Processed 390 frames / 390 total\n",
      "Processed 420 frames / 420 total\n",
      "Processed 450 frames / 450 total\n",
      "Processed 480 frames / 480 total\n",
      "Processed 510 frames / 510 total\n",
      "Processed 540 frames / 540 total\n",
      "Processed 570 frames / 570 total\n",
      "Processed 600 frames / 600 total\n",
      "Processed 630 frames / 630 total\n",
      "Processed 660 frames / 660 total\n",
      "Processed 690 frames / 690 total\n",
      "Processed 720 frames / 720 total\n",
      "Processed 750 frames / 750 total\n",
      "Processed 780 frames / 780 total\n",
      "Processed 810 frames / 810 total\n",
      "Processed 840 frames / 840 total\n",
      "Processed 870 frames / 870 total\n",
      "Processed 900 frames / 900 total\n",
      "Processed 930 frames / 930 total\n",
      "Processed 960 frames / 960 total\n",
      "Processed 990 frames / 990 total\n",
      "Processed 1020 frames / 1020 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 1035\n",
      "Processed frames: 1035\n",
      "Output saved to: output_videos/test_video3_cls_fixedcrop.mp4\n"
     ]
    }
   ],
   "source": [
    "real_time_classify(cls_model_path='runs/classify/train80/weights/best.pt', input_video='input_videos/test_video3.mp4', output_video='output_videos/test_video3_cls_fixedcrop.mp4', fixed_crop=True, line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fabb18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1080 x 1920 at 30.011688030155746 fps, 1035 frames\n",
      "Output: 1080 x 1920 at 30.011688030155746 fps\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 30 frames / 30 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 60 frames / 60 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 90 frames / 90 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 120 frames / 120 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 150 frames / 150 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 180 frames / 180 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 210 frames / 210 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 240 frames / 240 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 270 frames / 270 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 300 frames / 300 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 330 frames / 330 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 360 frames / 360 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 390 frames / 390 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 420 frames / 420 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 450 frames / 450 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 480 frames / 480 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 510 frames / 510 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 540 frames / 540 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 570 frames / 570 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 600 frames / 600 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 630 frames / 630 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 660 frames / 660 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 690 frames / 690 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 720 frames / 720 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 750 frames / 750 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 780 frames / 780 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 810 frames / 810 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 840 frames / 840 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 870 frames / 870 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 900 frames / 900 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 930 frames / 930 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 960 frames / 960 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 990 frames / 990 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Processed 1020 frames / 1020 total\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "cropping bounding box...\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 1035\n",
      "Processed frames: 1035\n",
      "Output saved to: output_videos/test_video3_cls_bbcrop.mp4\n"
     ]
    }
   ],
   "source": [
    "real_time_classify(cls_model='runs/classify/runs/train35', input_video='input_videos/test_video3.mp4', output_video='output_videos/test_video3_cls_bbcrop.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1bde8a",
   "metadata": {},
   "source": [
    "### Pose Estimation Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e4c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euclidean_distance(x1, y1, x2, y2):\n",
    "    '''\n",
    "        returns the euclidean distance between two points\n",
    "        \n",
    "        Args:\n",
    "            x1 (float): x coordinate of first point\n",
    "            y1 (float): y coordinate of first point\n",
    "            x2 (float): x coordinate of second point\n",
    "            y2 (float): y coordinate of second point\n",
    "            \n",
    "        Returns:\n",
    "            float: euclidean distance between the two points\n",
    "    '''\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "def get_video_properties(cap):\n",
    "    '''\n",
    "        returns the video properties\n",
    "        \n",
    "        Args:\n",
    "            cap (cv2.VideoCapture): video capture object\n",
    "            \n",
    "        Returns:\n",
    "            tuple: fps, width, height, total_frames\n",
    "    '''\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    return original_fps, width, height, total_frames\n",
    "\n",
    "def add_text_to_frame(display_text, annotated_frame, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=1.5, thickness=2, bgr=(218, 232, 0)):\n",
    "    '''\n",
    "        adds text to a frame\n",
    "        \n",
    "        Args:\n",
    "            display_text (str): text to be displayed\n",
    "            font (int): font type\n",
    "            font_scale (float): font scale\n",
    "            thickness (int): thickness of the text\n",
    "            annotated_frame (numpy.ndarray): frame to add text to\n",
    "            bgr (tuple): background color of the text\n",
    "    '''\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(display_text, font, font_scale, thickness)\n",
    "    line_height = text_height + baseline + 5\n",
    "    y_offset = 50\n",
    "\n",
    "    for i, line in enumerate(display_text.split('\\n')):\n",
    "        cv2.putText(annotated_frame, line, \n",
    "                        org=(10, y_offset + i * line_height), \n",
    "                        fontFace=font, \n",
    "                        fontScale=font_scale, \n",
    "                        color=bgr, \n",
    "                        thickness=thickness)\n",
    "        \n",
    "def calc_intersection(box1, box2):\n",
    "    '''\n",
    "        calculates the intersection of two bounding boxes\n",
    "        \n",
    "        Args:\n",
    "            box1 (tuple): bounding box coordinates (x1, y1, x2, y2)\n",
    "            box2 (tuple): bounding box coordinates (x1, y1, x2, y2)\n",
    "            \n",
    "        Returns:\n",
    "            float: intersection of the two bounding boxes\n",
    "    '''\n",
    "\n",
    "    x1, y1, x2, y2 = box1 \n",
    "    x3, y3, x4, y4 = box2\n",
    "\n",
    "    # calculate intersection area\n",
    "    intersection_x = min(x2, x4) - max(x1, x3)\n",
    "    intersection_y = min(y2, y4) - max(y1, y3)\n",
    "    intersection_area = intersection_x * intersection_y\n",
    "\n",
    "    return intersection_area\n",
    "    \n",
    "def get_classification(pose_model, det_model, frame):\n",
    "\n",
    "    det_results = det_model(frame, verbose=False)\n",
    "    pose_results = pose_model(frame, verbose=False)\n",
    "\n",
    "    light_coordinates = [] # stores tuples of ((x_mid, y_mid, width, height))\n",
    "    light_detected = False\n",
    "    l_mid_x, l_mid_y = 0, 0 # midpoint of light bounding box\n",
    "    annotated_frame = frame\n",
    "    total_mid_x, total_mid_y = 0, 0\n",
    "\n",
    "    if det_results and len(det_results) > 0:\n",
    "        det_result = det_results[0]\n",
    "        if det_result.boxes is not None and len(det_result.boxes) > 0:\n",
    "            annotated_frame = det_result.plot() # plot detection results on frame\n",
    "        \n",
    "            # calculate average midpoint of all light detections\n",
    "            class_indices = det_result.boxes.cls.int().cpu().numpy() if det_result.boxes.cls is not None else []\n",
    "            for i, class_idx in enumerate(class_indices):\n",
    "                class_name = det_model.names[class_idx]\n",
    "                    \n",
    "                if class_name == 'light':\n",
    "                    light_coordinates.append(det_result.boxes.xywh[i].cpu().numpy())\n",
    "                    light_detected = True\n",
    "                    \n",
    "    if light_detected:\n",
    "                \n",
    "        for l_x_mid, l_y_mid, _, _ in light_coordinates:\n",
    "            total_mid_x += l_x_mid\n",
    "            total_mid_y += l_y_mid\n",
    "        l_mid_x, l_mid_y = total_mid_x / len(light_coordinates), total_mid_y / len(light_coordinates)\n",
    "                \n",
    "        if pose_results is not None and len(pose_results) > 0:\n",
    "\n",
    "            total_left_shoulder_x, total_left_shoulder_y, total_right_shoulder_x, total_right_shoulder_y = 0, 0, 0, 0\n",
    "            left_eye_present, right_eye_present = False, False # used to determine if AP or outlet\n",
    "            valid_pose_detected = False\n",
    "            pose_count = 0\n",
    "            \n",
    "            result = pose_results[0]\n",
    "            annotated_frame = result.plot(img=annotated_frame) # plot pose estimation results on frame\n",
    "            if result.keypoints is not None and result.keypoints.data is not None:\n",
    "                keypoints_data = result.keypoints.data\n",
    "\n",
    "                keypoints = keypoints_data[0] # take first person\n",
    "\n",
    "                if keypoints.shape[0] >= 17: # ensure all keypoints present                   \n",
    "                    left_shoulder_x, left_shoulder_y = float(keypoints[5][0]), float(keypoints[5][1])\n",
    "                    right_shoulder_x, right_shoulder_y = float(keypoints[6][0]), float(keypoints[6][1])\n",
    "                                \n",
    "                    total_left_shoulder_x += left_shoulder_x\n",
    "                    total_left_shoulder_y += left_shoulder_y\n",
    "                    total_right_shoulder_x += right_shoulder_x \n",
    "                    total_right_shoulder_y += right_shoulder_y\n",
    "                                \n",
    "                    left_eye_conf = float(keypoints[1][2]) if keypoints.shape[0] > 1 else 0 \n",
    "                    right_eye_conf = float(keypoints[2][2]) if keypoints.shape[0] > 2 else 0\n",
    "                                    \n",
    "                    left_eye_present = left_eye_conf > 0.3\n",
    "                    right_eye_present = right_eye_conf > 0.3\n",
    "\n",
    "                    pose_count += 1\n",
    "                    valid_pose_detected = True\n",
    "                                    \n",
    "            if valid_pose_detected:\n",
    "                # calculate average shoulder coordinates\n",
    "                avg_left_shoulder_x = total_left_shoulder_x / pose_count\n",
    "                avg_left_shoulder_y = total_left_shoulder_y / pose_count\n",
    "                avg_right_shoulder_x = total_right_shoulder_x / pose_count                    \n",
    "                avg_right_shoulder_y = total_right_shoulder_y / pose_count\n",
    "                        \n",
    "                # add to distance between shoulders\n",
    "                dist_left = calc_euclidean_distance(avg_left_shoulder_x, avg_left_shoulder_y, l_mid_x, l_mid_y)\n",
    "                dist_right = calc_euclidean_distance(avg_right_shoulder_x, avg_right_shoulder_y, l_mid_x, l_mid_y)\n",
    "\n",
    "                # get final class\n",
    "                if dist_left < dist_right:\n",
    "                    class_name = 'left'\n",
    "                else:\n",
    "                    class_name = 'right'\n",
    "                                \n",
    "                if not left_eye_present or not right_eye_present:\n",
    "                    class_name += '_outlet'\n",
    "                else:\n",
    "                    class_name += '_AP'\n",
    "\n",
    "                display_text = (  \n",
    "                    f'Class: {class_name}\\n'\n",
    "                    f'Left Shoulder Distance: {dist_left:.2f} p\\n'\n",
    "                    f'Right Shoulder Distance: {dist_right:.2f} p\\n'\n",
    "                    f'Left Eye Present: {left_eye_present}\\n'\n",
    "                    f'Right Eye Present: {right_eye_present}'\n",
    "                )\n",
    "                        \n",
    "            else:\n",
    "                display_text = 'Pose detected but no shoulders'\n",
    "                class_name = 'unknown'\n",
    "        else:    \n",
    "            display_text = 'No valid pose detected'\n",
    "            class_name = 'unknown'\n",
    "    else:\n",
    "        display_text = 'No light detected'\n",
    "        class_name = 'unknown'\n",
    "\n",
    "    return annotated_frame, display_text, class_name\n",
    "\n",
    "def get_classification_v2(pose_model, det_model, frame):\n",
    "\n",
    "    det_results = det_model(frame, verbose=False)\n",
    "    pose_results = pose_model(frame, verbose=False)\n",
    "\n",
    "    light_coordinates = [] # stores tuples of ((l_x1, l_y1, lx2, ly2), (x_mid, y_mid, width, height))\n",
    "    light_detected = False\n",
    "    l_mid_x, l_mid_y = 0, 0 # midpoint of light bounding box\n",
    "    annotated_frame = frame\n",
    "    total_mid_x, total_mid_y = 0, 0\n",
    "\n",
    "    if det_results and len(det_results) > 0:\n",
    "        det_result = det_results[0]\n",
    "        if det_result.boxes is not None and len(det_result.boxes) > 0:\n",
    "            annotated_frame = det_result.plot() # plot detection results on frame\n",
    "        \n",
    "            # calculate average midpoint of all light detections\n",
    "            class_indices = det_result.boxes.cls.int().cpu().numpy() if det_result.boxes.cls is not None else []\n",
    "            for i, class_idx in enumerate(class_indices):\n",
    "                class_name = det_model.names[class_idx]\n",
    "                    \n",
    "                if class_name == 'light':\n",
    "                    light_coordinates.append((det_result.boxes.xyxy[i].cpu().numpy(), det_result.boxes.xywh[i].cpu().numpy()))\n",
    "                    light_detected = True\n",
    "                    \n",
    "    if light_detected:\n",
    "\n",
    "        if pose_results is not None and len(pose_results) > 0:\n",
    "\n",
    "            # get person bounding box\n",
    "            result = pose_results[0]\n",
    "            annotated_frame = result.plot(img=annotated_frame) # plot pose estimation results on frame\n",
    "\n",
    "            if result.boxes is not None and len(result.boxes) > 0:\n",
    "                p_x1, p_y1, p_x2, p_y2 = result.boxes.xyxy[0]\n",
    "\n",
    "            # calculate average midpoint of all light detections\n",
    "            intersect_count = 0\n",
    "            for (l_x1, l_y1, l_x2, l_y2), (l_x_mid, l_y_mid, w, h) in light_coordinates:\n",
    "                intersection_area = calc_intersection((l_x1, l_y1, l_x2, l_y2), (p_x1, p_y1, p_x2, p_y2)) # check for intersection\n",
    "                if intersection_area > 0: # if intersection area is greater than 0, add to total\n",
    "                    total_mid_x += l_x_mid\n",
    "                    total_mid_y += l_y_mid\n",
    "                    intersect_count += 1\n",
    "\n",
    "            if intersect_count == 0: # no x-ray light means no class\n",
    "                class_name = 'unknown'\n",
    "                display_text = 'No x-ray light detected'\n",
    "                return annotated_frame, display_text, class_name\n",
    "            \n",
    "            l_mid_x, l_mid_y = total_mid_x / intersect_count, total_mid_y / intersect_count # calc\n",
    "\n",
    "            total_left_shoulder_x, total_left_shoulder_y, total_right_shoulder_x, total_right_shoulder_y = 0, 0, 0, 0\n",
    "            left_eye_present, right_eye_present = False, False # used to determine if AP or outlet\n",
    "            valid_pose_detected = False\n",
    "            pose_count = 0\n",
    "            \n",
    "            if result.keypoints is not None and result.keypoints.data is not None:\n",
    "                keypoints_data = result.keypoints.data\n",
    "\n",
    "                keypoints = keypoints_data[0] # take first person\n",
    "\n",
    "                if keypoints.shape[0] >= 17: # ensure all keypoints present                   \n",
    "                    left_shoulder_x, left_shoulder_y = float(keypoints[5][0]), float(keypoints[5][1])\n",
    "                    right_shoulder_x, right_shoulder_y = float(keypoints[6][0]), float(keypoints[6][1])\n",
    "                                \n",
    "                    total_left_shoulder_x += left_shoulder_x\n",
    "                    total_left_shoulder_y += left_shoulder_y\n",
    "                    total_right_shoulder_x += right_shoulder_x \n",
    "                    total_right_shoulder_y += right_shoulder_y\n",
    "                                \n",
    "                    left_eye_conf = float(keypoints[1][2]) if keypoints.shape[0] > 1 else 0 \n",
    "                    right_eye_conf = float(keypoints[2][2]) if keypoints.shape[0] > 2 else 0\n",
    "                                    \n",
    "                    left_eye_present = left_eye_conf > 0.3\n",
    "                    right_eye_present = right_eye_conf > 0.3\n",
    "\n",
    "                    pose_count += 1\n",
    "                    valid_pose_detected = True\n",
    "                                    \n",
    "            if valid_pose_detected:\n",
    "                # calculate average shoulder coordinates\n",
    "                avg_left_shoulder_x = total_left_shoulder_x / pose_count\n",
    "                avg_left_shoulder_y = total_left_shoulder_y / pose_count\n",
    "                avg_right_shoulder_x = total_right_shoulder_x / pose_count                    \n",
    "                avg_right_shoulder_y = total_right_shoulder_y / pose_count\n",
    "                        \n",
    "                # add to distance between shoulders\n",
    "                dist_left = calc_euclidean_distance(avg_left_shoulder_x, avg_left_shoulder_y, l_mid_x, l_mid_y)\n",
    "                dist_right = calc_euclidean_distance(avg_right_shoulder_x, avg_right_shoulder_y, l_mid_x, l_mid_y)\n",
    "\n",
    "                # get final class\n",
    "                if dist_left < dist_right:\n",
    "                    class_name = 'left'\n",
    "                else:\n",
    "                    class_name = 'right'\n",
    "                                \n",
    "                if not left_eye_present or not right_eye_present:\n",
    "                    class_name += '_outlet'\n",
    "                else:\n",
    "                    class_name += '_AP'\n",
    "\n",
    "                display_text = (  \n",
    "                    f'Class: {class_name}\\n'\n",
    "                    f'Left Shoulder Distance: {dist_left:.2f} p\\n'\n",
    "                    f'Right Shoulder Distance: {dist_right:.2f} p\\n'\n",
    "                    f'Left Eye Present: {left_eye_present}\\n'\n",
    "                    f'Right Eye Present: {right_eye_present}'\n",
    "                )\n",
    "                        \n",
    "            else:\n",
    "                display_text = 'Pose detected but no shoulders'\n",
    "                class_name = 'unknown'\n",
    "        else:    \n",
    "            display_text = 'No valid pose detected'\n",
    "            class_name = 'unknown'\n",
    "    else:\n",
    "        display_text = 'No light detected'\n",
    "        class_name = 'unknown'\n",
    "\n",
    "    return annotated_frame, display_text, class_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def video_pose_light_classify(pose_model_path, det_model_path, video_path, output_path, display_frame=False):\n",
    "    '''\n",
    "        real time classification using pose estimation and light detection\n",
    "        \n",
    "        Args:\n",
    "            pose_model_path (str): path to the pose estimation model\n",
    "            det_model_path (str): path to the light detection model\n",
    "            video_path (str): path to the video file\n",
    "            output_path (str): path to the output video file\n",
    "            display_frame (bool): whether to display the frame or not\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    '''\n",
    "    # load models\n",
    "    pose_model = YOLO(pose_model_path)\n",
    "    det_model = YOLO(det_model_path)\n",
    "\n",
    "    # open video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # get video properties\n",
    "    original_fps, width, height, total_frames = get_video_properties(cap)\n",
    "\n",
    "    print(f\"Video properties: fps={original_fps}, width={width}, height={height}, total frames={total_frames}\")\n",
    "\n",
    "    # initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, original_fps, (width, height))\n",
    "\n",
    "    if not video_writer.isOpened():\n",
    "        print('Error opening video writer')\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # process video\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print('Video frame is empty or video processing successfully completed')\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            annotated_frame, display_text, class_name = get_classification_v2(pose_model, det_model, frame)\n",
    "                \n",
    "            # add text to frame\n",
    "            add_text_to_frame(display_text=display_text, annotated_frame=annotated_frame)\n",
    "                \n",
    "            if annotated_frame.shape[:2] != (height, width):\n",
    "                annotated_frame = cv2.resize(annotated_frame, (width, height))\n",
    "        \n",
    "            video_writer.write(annotated_frame) # save frame to output video\n",
    "            processed_count += 1\n",
    "\n",
    "            # print progress\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {processed_count} frames / {frame_count} total\")\n",
    "\n",
    "            # display frame in real time if requested\n",
    "            if display_frame: \n",
    "                final_frame = resize_with_aspect_ratio(annotated_frame, width=500, height=500)\n",
    "                \n",
    "                cv2.imshow('YOLO11 Real-time', final_frame)\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "                if key == ord('q'):\n",
    "                    print('Processing stopped by user')\n",
    "                    break\n",
    "                elif key == ord('p'):\n",
    "                    print('Processing paused, press any key to resume')\n",
    "                    cv2.waitKey(0)\n",
    "                    print('Processing resumed')\n",
    "\n",
    "    except Exception as e:\n",
    "        e.with_traceback(e)\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        cap.release()\n",
    "        video_writer.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print('Video processing completed')\n",
    "        print(f'Input frames: {frame_count}')\n",
    "        print(f'Processed frames: {processed_count}')\n",
    "        print(f'Output saved to: {output_path}')\n",
    "        print('Inference speed: ', ((end_time - start_time) * 10**3) / frame_count, 'ms/frame')\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b89c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_pose_light_classify(pose_model_path, det_model_path, img_path, output_path):\n",
    "    '''\n",
    "        real time classification using pose estimation and x-ray light detection. saves images in input_path, and saves to output_path\n",
    "        \n",
    "        Args:\n",
    "            pose_model_path (str): path to the pose estimation model\n",
    "            det_model_path (str): path to the x-ray light detection model\n",
    "            images_path (str): path to the images directory\n",
    "            output_path (str): path to the output video file\n",
    "            display_frame (bool): whether to display the frame or not\n",
    "\n",
    "        Returns:    \n",
    "            None\n",
    "    '''\n",
    "\n",
    "    # load models\n",
    "    \n",
    "    pose_model =  YOLO(pose_model_path)\n",
    "    det_model = YOLO(det_model_path)\n",
    "\n",
    "    # iterate through images and apply pose_estimation and light detection to classify images\n",
    "\n",
    "    # load image\n",
    "    frame = cv2.imread(img_path)\n",
    "\n",
    "    annotated_frame, display_text, class_name = get_classification(pose_model, det_model, frame)\n",
    "\n",
    "    add_text_to_frame(font_scale=0.5, display_text=display_text, annotated_frame=annotated_frame)\n",
    "\n",
    "    # save image\n",
    "    cv2.imwrite(output_path, annotated_frame)\n",
    "\n",
    "    return class_name\n",
    "\n",
    "def pose_detect_batch_classify(input_dir, output_dir, pose_model_path, det_model_path, classes):\n",
    "    '''\n",
    "        classify images in input_dir using pose estimation and x-ray light detection. saves images in output_dir \n",
    "        and prints the per class accuracy\n",
    "        \n",
    "        Args:\n",
    "            input_dir (str): path to the input directory\n",
    "            output_dir (str): path to the output directory\n",
    "            pose_model_path (str): path to the pose estimation model\n",
    "            det_model_path (str): path to the x-ray light detection model\n",
    "\n",
    "        Returns:    \n",
    "            None\n",
    "    '''\n",
    "    total_processed = 0\n",
    "    total_correct = 0\n",
    "    for class_name in classes:\n",
    "        curr_input_path = os.path.join(input_dir, class_name)\n",
    "        curr_output_path = os.path.join(output_dir, class_name)\n",
    "        shutil.rmtree(curr_output_path, ignore_errors=True)\n",
    "        os.makedirs(curr_output_path, exist_ok=True)\n",
    "\n",
    "        print('Processing class:', class_name, 'from', input_dir)\n",
    "    \n",
    "        imgs = [img for img in os.listdir(curr_input_path) if img.endswith('.jpg')]\n",
    "\n",
    "        total_images = len(imgs)\n",
    "        correct_classified_count = 0\n",
    "        processed_count = 0 \n",
    "\n",
    "        print(f'Total images: {total_images}')\n",
    "        for img in imgs:\n",
    "            img_path = os.path.join(curr_input_path, img)\n",
    "            img_output_path = os.path.join(curr_output_path, img)\n",
    "            predicted_class_name = image_pose_light_classify(pose_model_path=pose_model_path, det_model_path=det_model_path, \n",
    "                                                img_path=img_path, output_path=img_output_path)\n",
    "            \n",
    "            if predicted_class_name == class_name:\n",
    "                correct_classified_count += 1\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            if processed_count % 30 == 0:\n",
    "                print(f'Processed {processed_count} of {total_images}')\n",
    "        \n",
    "        print(f'Correctly classified images: {correct_classified_count}')\n",
    "        print(f'Class accuracy: {correct_classified_count / total_images}')\n",
    "        \n",
    "        total_processed += total_images\n",
    "        total_correct += correct_classified_count\n",
    "    \n",
    "    print(f'Total processed: {total_processed}')\n",
    "    print(f'Total correct: {total_correct}')\n",
    "    print(f'Total Accuracy: {total_correct / total_processed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4e4002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video properties: fps=30.21165555233401, width=1080, height=1920, total frames=521\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Processed 360 frames / 360 total\n",
      "Processed 390 frames / 390 total\n",
      "Processed 420 frames / 420 total\n",
      "Processed 450 frames / 450 total\n",
      "Processed 480 frames / 480 total\n",
      "Processed 510 frames / 510 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 521\n",
      "Processed frames: 521\n",
      "Output saved to: output_videos/test_video2_light.mp4\n",
      "Inference speed:  131.9465261960899 ms/frame\n"
     ]
    }
   ],
   "source": [
    "video_pose_light_classify(det_model_path='runs/detect/train_light35deg/weights/best.pt', pose_model_path='yolo11l-pose.pt', video_path='input_videos/test_video2.mp4', output_path='output_videos/test_video2_light.mp4', display_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb98ed37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video properties: fps=30.011688030155746, width=1080, height=1920, total frames=1035\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Processed 360 frames / 360 total\n",
      "Processed 390 frames / 390 total\n",
      "Processed 420 frames / 420 total\n",
      "Processed 450 frames / 450 total\n",
      "Processed 480 frames / 480 total\n",
      "Processed 510 frames / 510 total\n",
      "Processed 540 frames / 540 total\n",
      "Processed 570 frames / 570 total\n",
      "Processed 600 frames / 600 total\n",
      "Processed 630 frames / 630 total\n",
      "Processed 660 frames / 660 total\n",
      "Processed 690 frames / 690 total\n",
      "Processed 720 frames / 720 total\n",
      "Processed 750 frames / 750 total\n",
      "Processed 780 frames / 780 total\n",
      "Processed 810 frames / 810 total\n",
      "Processed 840 frames / 840 total\n",
      "Processed 870 frames / 870 total\n",
      "Processed 900 frames / 900 total\n",
      "Processed 930 frames / 930 total\n",
      "Processed 960 frames / 960 total\n",
      "Processed 990 frames / 990 total\n",
      "Processed 1020 frames / 1020 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 1035\n",
      "Processed frames: 1035\n",
      "Output saved to: output_videos/test_video3_light.mp4\n",
      "Inference speed:  88.4408093880916 ms/frame\n"
     ]
    }
   ],
   "source": [
    "video_pose_light_classify(det_model_path='runs/detect/train_light/weights/best.pt', pose_model_path='yolo11l-pose.pt', video_path='input_videos/test_video3.mp4', output_path='output_videos/test_video3_light.mp4', display_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a42ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video properties: fps=30.0, width=1080, height=1920, total frames=333\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 333\n",
      "Processed frames: 333\n",
      "Output saved to: output_videos/test_video4_light.mp4\n",
      "Inference speed:  91.70113741098581 ms/frame\n"
     ]
    }
   ],
   "source": [
    "video_pose_light_classify(det_model_path='runs/detect/train_light/weights/best.pt', pose_model_path='yolo11l-pose.pt', video_path='input_videos/test_video4.mp4', output_path='output_videos/test_video4_light.mp4', display_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebeca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video properties: fps=30.0, width=1080, height=1920, total frames=337\n",
      "Processed 30 frames / 30 total\n",
      "Processed 60 frames / 60 total\n",
      "Processed 90 frames / 90 total\n",
      "Processed 120 frames / 120 total\n",
      "Processed 150 frames / 150 total\n",
      "Processed 180 frames / 180 total\n",
      "Processed 210 frames / 210 total\n",
      "Processed 240 frames / 240 total\n",
      "Processed 270 frames / 270 total\n",
      "Processed 300 frames / 300 total\n",
      "Processed 330 frames / 330 total\n",
      "Video frame is empty or video processing successfully completed\n",
      "Video processing completed\n",
      "Input frames: 337\n",
      "Processed frames: 337\n",
      "Output saved to: output_videos/test_video5_light.mp4\n",
      "Inference speed:  103.86039029243085 ms/frame\n"
     ]
    }
   ],
   "source": [
    "video_pose_light_classify(det_model_path='runs/detect/train_light/weights/best.pt', pose_model_path='yolo11l-pose.pt', video_path='input_videos/test_video5.mp4', output_path='output_videos/test_video5_light.mp4', display_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4532df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: left_outlet from original_data/train\n",
      "Total images: 402\n",
      "Processed 30 of 402\n",
      "Processed 60 of 402\n",
      "Processed 90 of 402\n",
      "Processed 120 of 402\n",
      "Processed 150 of 402\n",
      "Processed 180 of 402\n",
      "Processed 210 of 402\n",
      "Processed 240 of 402\n",
      "Processed 270 of 402\n",
      "Processed 300 of 402\n",
      "Processed 330 of 402\n",
      "Processed 360 of 402\n",
      "Processed 390 of 402\n",
      "Correctly classified images: 336\n",
      "Class accuracy: 0.835820895522388\n",
      "Processing class: right_outlet from original_data/train\n",
      "Total images: 419\n",
      "Processed 30 of 419\n",
      "Processed 60 of 419\n",
      "Processed 90 of 419\n",
      "Processed 120 of 419\n",
      "Processed 150 of 419\n",
      "Processed 180 of 419\n",
      "Processed 210 of 419\n",
      "Processed 240 of 419\n",
      "Processed 270 of 419\n",
      "Processed 300 of 419\n",
      "Processed 330 of 419\n",
      "Processed 360 of 419\n",
      "Processed 390 of 419\n",
      "Correctly classified images: 417\n",
      "Class accuracy: 0.9952267303102625\n",
      "Total processed: 821\n",
      "Total correct: 753\n",
      "Total Accuracy: 0.9171741778319124\n"
     ]
    }
   ],
   "source": [
    "# try on all training images\n",
    "classes = ['left_outlet', 'right_outlet']\n",
    "\n",
    "pose_detect_batch_classify(classes=classes, input_dir='original_data/train', output_dir='pose_detect_images/train_posex', pose_model_path='yolo11x-pose.pt', \n",
    "                           det_model_path='runs/detect/train_light35deg/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c52c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: left_outlet from test/test/left_outlet\n",
      "Total images: 133\n",
      "Processed 30 of 133\n",
      "Processed 60 of 133\n",
      "Processed 90 of 133\n",
      "Processed 120 of 133\n",
      "Correctly classified images: 126\n",
      "Class accuracy: 0.9473684210526315\n",
      "Processing class: right_outlet from test/test/right_outlet\n",
      "Total images: 134\n",
      "Processed 30 of 134\n",
      "Processed 60 of 134\n",
      "Processed 90 of 134\n",
      "Processed 120 of 134\n",
      "Correctly classified images: 134\n",
      "Class accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# try on all test images\n",
    "classes = ['left_AP', 'right_AP', 'left_outlet', 'right_outlet']\n",
    "\n",
    "for class_name in classes:\n",
    "    input_dir = f'test/test/{class_name}'\n",
    "    output_dir = f'pose_detect_images/test_posex/{class_name}'\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print('Processing class:', class_name, 'from', input_dir)\n",
    "   \n",
    "    imgs = [img for img in os.listdir(input_dir) if img.endswith('.jpg')]\n",
    "\n",
    "    total_images = len(imgs)\n",
    "    correct_classified_count = 0\n",
    "    processed_count = 0 \n",
    "\n",
    "    print(f'Total images: {total_images}')\n",
    "    for img in imgs:\n",
    "        img_path = os.path.join(input_dir, img)\n",
    "        output_path = os.path.join(output_dir, img)\n",
    "        predicted_class_name = image_pose_light_classify(pose_model_path='yolo11x-pose.pt', det_model_path='runs/detect/train_light35deg/weights/best.pt', \n",
    "                                               img_path=img_path, output_path=output_path)\n",
    "        \n",
    "        if predicted_class_name == class_name:\n",
    "            correct_classified_count += 1\n",
    "\n",
    "        processed_count += 1\n",
    "\n",
    "        if processed_count % 30 == 0:\n",
    "            print(f'Processed {processed_count} of {total_images}')\n",
    "    \n",
    "    print(f'Correctly classified images: {correct_classified_count}')\n",
    "    print(f'Class accuracy: {correct_classified_count / total_images}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe7d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: left_AP from test/test\n",
      "Total images: 45\n",
      "Processed 30 of 45\n",
      "Correctly classified images: 45\n",
      "Class accuracy: 1.0\n",
      "Processing class: right_AP from test/test\n",
      "Total images: 30\n",
      "Processed 30 of 30\n",
      "Correctly classified images: 30\n",
      "Class accuracy: 1.0\n",
      "Total processed: 75\n",
      "Total correct: 75\n",
      "Total Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "pose_detect_batch_classify(input_dir='test/test', output_dir='pose_detect_images/test_posex', classes = ['left_AP', 'right_AP'], \n",
    "                           pose_model_path='yolo11x-pose.pt', det_model_path='runs/detect/train_light35deg/weights/best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e846614",
   "metadata": {},
   "source": [
    "### Pose Estimation using Mediapipe ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2596e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tatks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE\n",
    ")\n",
    "\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input image to mediapipe.Image object\n",
    "def create_mp_image('str')\n",
    "mp_image = mp.Image.create_from_file('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
